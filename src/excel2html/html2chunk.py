from pathlib import Path
from bs4 import BeautifulSoup
import copy


def estimate_tokens(text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆä¸­æ–‡çº¦2.5å­—ç¬¦=1tokenï¼‰"""
    return int(len(text) / 2.5)


def distribute_assets_and_chunk(
    long_html_content, max_rows_per_chunk: int = None, max_tokens_per_chunk: int = None
):
    """
    æ ¸å¿ƒé€»è¾‘ï¼šå°†é•¿ HTML åˆ‡åˆ†ï¼Œå¹¶æŠŠå…¨å±€èµ„äº§ï¼ˆContext/Caption/Headerï¼‰åˆ†å‘ç»™æ¯ä¸ªç‰‡æ®µ

    å‚æ•°:
        long_html_content: å®Œæ•´ HTML å†…å®¹
        max_rows_per_chunk: æŒ‰è¡Œæ•°åˆ‡åˆ†ï¼ˆä¼˜å…ˆï¼‰
        max_tokens_per_chunk: æŒ‰ token æ•°åˆ‡åˆ†ï¼ˆæ›´ç²¾ç¡®ï¼‰

    å¦‚æœä¸¤ä¸ªå‚æ•°éƒ½æœªæŒ‡å®šï¼Œé»˜è®¤ max_rows_per_chunk=8
    """
    # é»˜è®¤å€¼
    if max_rows_per_chunk is None and max_tokens_per_chunk is None:
        max_rows_per_chunk = 8

    soup = BeautifulSoup(long_html_content, "html.parser")

    # 1. æå–å…¨å±€èµ„äº§ (Context Div)
    context_div = soup.find("div", class_="rag-context")
    if not context_div:
        table_node = soup.find("table")
        if table_node and table_node.find_previous_sibling("div"):
            context_div = table_node.find_previous_sibling("div")

    # 2. æå–è¡¨æ ¼æ ¸å¿ƒç»„ä»¶
    original_table = soup.find("table")
    if not original_table:
        return [long_html_content]

    caption = original_table.find("caption")

    header_rows = []
    thead = original_table.find("thead")
    if thead:
        header_rows = thead.find_all("tr")
    else:
        header_rows = original_table.find_all("tr")[:1]

    # 3. å‡†å¤‡æ•°æ®è¡Œ
    tbody = original_table.find("tbody")
    if tbody:
        data_rows = tbody.find_all("tr")
    else:
        all_rows = original_table.find_all("tr")
        data_rows = [row for row in all_rows if row not in header_rows]

    # 4. è®¡ç®—å›ºå®šå¼€é”€ï¼ˆç”¨äº token æ¨¡å¼ï¼‰
    fixed_parts = []
    if context_div:
        fixed_parts.append(str(context_div))
    if caption:
        fixed_parts.append(str(caption))
    for h_row in header_rows:
        fixed_parts.append(str(h_row))
    fixed_overhead = estimate_tokens("".join(fixed_parts))

    chunks = []
    current_chunk_data = []
    current_chunk_tokens = 0

    def should_split(row_count, row_tokens):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡åˆ†"""
        if max_tokens_per_chunk is not None:
            # Token æ¨¡å¼ï¼šæ£€æŸ¥ç´¯è®¡ token æ˜¯å¦è¶…é™
            return (
                current_chunk_tokens + row_tokens + fixed_overhead
            ) > max_tokens_per_chunk
        else:
            # è¡Œæ•°æ¨¡å¼
            return row_count >= max_rows_per_chunk

    def build_chunk(data_rows_for_chunk):
        """ç»„è£…ä¸€ä¸ª chunk"""
        new_soup = BeautifulSoup("<div></div>", "html.parser")
        wrapper_div = new_soup.div

        if context_div:
            wrapper_div.append(copy.copy(context_div))

        new_table = new_soup.new_tag("table")
        new_table.attrs = original_table.attrs
        new_table["border"] = "1"
        new_table["style"] = "border-collapse:collapse"

        if caption:
            new_table.append(copy.copy(caption))

        new_thead = new_soup.new_tag("thead")
        for h_row in header_rows:
            new_thead.append(copy.copy(h_row))
        new_table.append(new_thead)

        new_tbody = new_soup.new_tag("tbody")
        for d_row in data_rows_for_chunk:
            new_tbody.append(copy.copy(d_row))
        new_table.append(new_tbody)

        wrapper_div.append(new_table)
        return str(wrapper_div)

    # 5. é€è¡Œç´¯åŠ åˆ‡åˆ†
    for i, row in enumerate(data_rows):
        row_tokens = estimate_tokens(str(row))

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å…ˆåˆ‡åˆ†ï¼ˆå½“å‰ chunk éç©ºä¸”åŠ å…¥æ–°è¡Œä¼šè¶…é™ï¼‰
        if current_chunk_data and should_split(len(current_chunk_data), row_tokens):
            chunks.append(build_chunk(current_chunk_data))
            current_chunk_data = []
            current_chunk_tokens = 0

        current_chunk_data.append(row)
        current_chunk_tokens += row_tokens

        # æœ€åä¸€è¡Œï¼Œæ”¶å°¾
        if i == len(data_rows) - 1 and current_chunk_data:
            chunks.append(build_chunk(current_chunk_data))

    return chunks


def process_and_merge_html(file_path_str, separator="!!!_CHUNK_BREAK_!!!"):
    """
    è¯»å–æ–‡ä»¶ -> åˆ‡åˆ† -> åˆå¹¶ -> ä¿å­˜
    """
    source_path = Path(file_path_str)

    if not source_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{source_path}'")
        return

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–: {source_path.name}")
    content = source_path.read_text(encoding="utf-8")

    # 1. æ‰§è¡Œåˆ‡åˆ†é€»è¾‘
    # å»ºè®® max_rows_per_chunk è®¾ç½®ä¸º 5-10ï¼Œä¿è¯æ¯ä¸ª chunk ä¸ä¼šå› ä¸ºåŠ ä¸Šè¡¨å¤´å’Œcontextåè¶…è¿‡ Token é™åˆ¶
    chunks = distribute_assets_and_chunk(content, max_rows_per_chunk=2)

    print(f"ğŸ”ª åˆ‡åˆ†å®Œæˆï¼šå…±ç”Ÿæˆ {len(chunks)} ä¸ªç‰‡æ®µ")

    # 2. æ‰§è¡Œåˆå¹¶é€»è¾‘
    # æˆ‘ä»¬åœ¨åˆ†éš”ç¬¦å‰ååŠ æ¢è¡Œç¬¦ï¼Œç¡®ä¿ç»“æ„æ¸…æ™°ï¼Œä¸ä¼šç²˜è¿ HTML æ ‡ç­¾
    formatted_separator = f"\n\n{separator}\n\n"
    merged_content = formatted_separator.join(chunks)

    # 3. æ„å»ºè¾“å‡ºè·¯å¾„
    # ä¾‹å­: input.html -> input_chunk_merged.html
    new_filename = source_path.stem + "_chunk_merged" + source_path.suffix
    output_path = source_path.with_name(new_filename)

    # 4. å†™å…¥æ–‡ä»¶
    try:
        output_path.write_text(merged_content, encoding="utf-8")
        print(f"âœ… åˆå¹¶æˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path.absolute()}")
        print(f"ğŸ”‘ ä½¿ç”¨çš„åˆ†éš”ç¬¦: {separator}")
    except IOError as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # è¯·å°†æ­¤å¤„ä¿®æ”¹ä¸ºä½ é‚£ä¸ªâ€œå·²ç»å¤„ç†è¿‡Contextçš„é•¿HTMLâ€æ–‡ä»¶è·¯å¾„
    my_input_file = "Files\Excel\æœ¬å›½å­ç›®æ³¨é‡Šè°ƒæ•´è¡¨.html"

    # è¿è¡Œ
    process_and_merge_html(my_input_file)
