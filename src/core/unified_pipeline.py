"""
ç»Ÿä¸€è½¬æ¢æµæ°´çº¿
æ”¯æŒ HTMLã€Markdownã€CSV å¤šç§è¾“å‡ºæ ¼å¼
"""

from dataclasses import dataclass
from pathlib import Path

from loguru import logger

from .base_converter import BaseExcelConverter
from .models import ChunkConfig, ConversionResult, OutputFormat, SplitMode


@dataclass
class UnifiedPipeline:
    """ç»Ÿä¸€è½¬æ¢æµæ°´çº¿"""

    output_format: OutputFormat = OutputFormat.HTML
    keywords: list[str] | None = None
    max_rows_per_chunk: int | None = None
    target_tokens: int = 1024
    separator: str = "!!!_CHUNK_BREAK_!!!"

    # MD ç‰¹å®šé€‰é¡¹
    md_include_metadata: bool = True

    def run(self, excel_path: Path) -> ConversionResult | None:
        """æ‰§è¡Œè½¬æ¢æµæ°´çº¿"""
        source_path = Path(excel_path) if not isinstance(excel_path, Path) else excel_path

        if not source_path.exists():
            logger.error(f"æ‰¾ä¸åˆ°æ–‡ä»¶ '{source_path}'")
            return None

        self._log_start(source_path)

        # ç¬¬ä¸€æ­¥ï¼šè½¬æ¢
        converter = self._create_converter()
        output_path = converter.convert(source_path)

        if not output_path:
            logger.error("æµæ°´çº¿ä¸­æ–­ï¼šè½¬æ¢å¤±è´¥")
            return None

        # ç¬¬äºŒæ­¥ï¼šåˆ‡åˆ†
        return self._process_with_chunking(output_path, source_path)

    def _log_start(self, source_path: Path) -> None:
        """è®°å½•å¼€å§‹æ—¥å¿—"""
        logger.info("=" * 50)
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æµæ°´çº¿: {source_path.name}")
        logger.info(f"ğŸ“„ è¾“å‡ºæ ¼å¼: {self.output_format.value.upper()}")
        logger.info("=" * 50)

    def _create_converter(self) -> BaseExcelConverter:
        """åˆ›å»ºå¯¹åº”æ ¼å¼çš„è½¬æ¢å™¨"""
        if self.output_format == OutputFormat.HTML:
            from .excel2html.converter import ExcelToHtmlConverter
            return ExcelToHtmlConverter(keywords=self.keywords)
        else:
            from .excel2md.converter import MarkdownConverter
            return MarkdownConverter(
                keywords=self.keywords,
                include_metadata=self.md_include_metadata,
            )

    def _process_with_chunking(self, output_path: Path, source_path: Path) -> ConversionResult | None:
        """å¤„ç†éœ€è¦åˆ‡åˆ†çš„æ ¼å¼"""
        logger.info("ğŸ“Œ ç¬¬äºŒæ­¥ï¼šåˆ‡åˆ†ä¸º Chunks")

        content = output_path.read_text(encoding="utf-8")
        config = self._build_chunk_config()

        if self.output_format == OutputFormat.HTML:
            from .excel2html.chunker import HtmlChunker
            chunker = HtmlChunker(config=config)
        else:
            from .excel2md.chunker import MarkdownChunker
            chunker = MarkdownChunker(config=config)

        result = chunker.chunk(content)
        self._log_chunk_result(result)

        # ä¿å­˜ç»“æœ
        ext = ".html" if self.output_format == OutputFormat.HTML else ".md"
        chunk_path = source_path.parent / f"{source_path.stem}{ext}"
        return self._save_chunks(chunk_path, result, output_path)

    def _build_chunk_config(self) -> ChunkConfig:
        """æ„å»ºåˆ‡åˆ†é…ç½®"""
        if self.max_rows_per_chunk is None:
            logger.info(f"ğŸ“Š ä½¿ç”¨ token æ¨¡å¼ï¼Œç›®æ ‡æ¯ chunk â‰¤ {self.target_tokens} tokens")
            return ChunkConfig(
                split_mode=SplitMode.BY_TOKENS,
                max_tokens=self.target_tokens,
                separator=self.separator,
            )
        else:
            logger.info(f"ğŸ“Š ä½¿ç”¨è¡Œæ•°æ¨¡å¼ï¼Œæ¯ chunk {self.max_rows_per_chunk} è¡Œ")
            return ChunkConfig(
                split_mode=SplitMode.BY_ROWS,
                max_rows=self.max_rows_per_chunk,
                separator=self.separator,
            )

    def _log_chunk_result(self, result) -> None:
        """è®°å½•åˆ‡åˆ†ç»“æœ"""
        stats = result.stats
        logger.info(f"ğŸ”ª åˆ‡åˆ†å®Œæˆï¼šå…±ç”Ÿæˆ {stats.total_chunks} ä¸ªç‰‡æ®µ")
        logger.info(
            f"ğŸ“Š Token ç»Ÿè®¡: æœ€å°={stats.min_token_count}, "
            f"æœ€å¤§={stats.max_token_count}, å¹³å‡={stats.avg_token_count:.1f}"
        )

        if result.warnings:
            logger.warning(f"æœ‰ {len(result.warnings)} ä¸ªç‰‡æ®µè¶…è¿‡ token é™åˆ¶ï¼š")
            for w in result.warnings:
                logger.warning(
                    f"   - ç‰‡æ®µ #{w.chunk_index}: {w.actual_tokens} tokens (è¶…å‡º {w.overflow})"
                )
                logger.warning(f"     åŸå› : {w.reason}")

    def _save_chunks(self, chunk_path: Path, result, output_path: Path) -> ConversionResult | None:
        """ä¿å­˜åˆ‡åˆ†ç»“æœ"""
        formatted_separator = f"\n\n{self.separator}\n\n"
        merged_content = formatted_separator.join(result.chunks)

        try:
            chunk_path.write_text(merged_content, encoding="utf-8")
            logger.info(f"âœ… Chunk æ–‡ä»¶å·²ä¿å­˜: {chunk_path.absolute()}")
        except OSError as e:
            logger.error(f"å†™å…¥ Chunk æ–‡ä»¶å¤±è´¥: {e}")
            return None

        self._log_completion(output_path, chunk_path, len(result.chunks))

        return ConversionResult(
            output_path=output_path,
            chunk_path=chunk_path,
            chunk_count=len(result.chunks),
            status_message="å¤„ç†å®Œæˆ",
            output_format=self.output_format,
            success=True,
            chunk_stats=result.stats,
        )

    def _log_completion(self, output_path: Path, chunk_path: Path, chunk_count: int) -> None:
        """è®°å½•å®Œæˆæ—¥å¿—"""
        logger.info("=" * 50)
        logger.info("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        logger.info(f"   ğŸ“„ ä¸­é—´ç»“æœ: {output_path}")
        logger.info(f"   ğŸ“„ æœ€ç»ˆç»“æœ: {chunk_path}")
        logger.info(f"   ğŸ”¢ Chunk æ•°é‡: {chunk_count}")
        logger.info(f"   ğŸ“ è¾“å‡ºæ ¼å¼: {self.output_format.value.upper()}")
        logger.info("=" * 50)


def run_unified_pipeline(
    excel_path: str,
    output_format: str = "html",
    keywords: list[str] | None = None,
    max_rows_per_chunk: int | None = None,
    target_tokens: int = 1024,
    separator: str = "!!!_CHUNK_BREAK_!!!",
) -> dict | None:
    """æ‰§è¡Œç»Ÿä¸€æµæ°´çº¿ï¼ˆå…¼å®¹å‡½æ•°æ¥å£ï¼‰"""
    format_map = {
        "html": OutputFormat.HTML,
        "md": OutputFormat.MARKDOWN,
        "markdown": OutputFormat.MARKDOWN,
    }

    fmt = format_map.get(output_format.lower(), OutputFormat.HTML)

    pipeline = UnifiedPipeline(
        output_format=fmt,
        keywords=keywords,
        max_rows_per_chunk=max_rows_per_chunk,
        target_tokens=target_tokens,
        separator=separator,
    )

    result = pipeline.run(Path(excel_path))

    if result is None:
        return None

    return {
        "output_path": str(result.output_path),
        "chunk_path": str(result.chunk_path),
        "chunk_count": result.chunk_count,
        "output_format": result.output_format.value,
    }
