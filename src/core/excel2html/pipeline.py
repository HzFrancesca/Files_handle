"""
Excel è½¬ HTML å®Œæ•´æµæ°´çº¿
è¾“å…¥ Excel æ–‡ä»¶ -> ç”Ÿæˆå¢å¼º HTML -> åˆ‡åˆ†ä¸º Chunks
"""

import argparse
from dataclasses import dataclass
from pathlib import Path

from loguru import logger

from ..models import ChunkConfig, ConversionResult, SplitMode
from .chunker import HtmlChunker
from .converter import ExcelToHtmlConverter


@dataclass
class ConversionPipeline:
    """Excel è½¬ HTML è½¬æ¢æµæ°´çº¿"""

    keywords: list[str] | None = None
    max_rows_per_chunk: int | None = None
    target_tokens: int = 1024
    separator: str = "!!!_CHUNK_BREAK_!!!"

    def run(self, excel_path: Path) -> ConversionResult | None:
        """æ‰§è¡Œå®Œæ•´çš„è½¬æ¢æµæ°´çº¿"""
        source_path = Path(excel_path) if not isinstance(excel_path, Path) else excel_path

        if not source_path.exists():
            logger.error(f"æ‰¾ä¸åˆ°æ–‡ä»¶ '{source_path}'")
            return None

        self._log_start(source_path)

        # ç¬¬ä¸€æ­¥ï¼šExcel -> HTML
        html_path = self._convert_to_html(source_path)
        if not html_path:
            return None

        # ç¬¬äºŒæ­¥ï¼šHTML -> Chunks
        chunk_result = self._chunk_html(html_path, source_path)
        if not chunk_result:
            return None

        return chunk_result

    def _log_start(self, source_path: Path) -> None:
        """è®°å½•å¼€å§‹æ—¥å¿—"""
        logger.info("=" * 50)
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æµæ°´çº¿: {source_path.name}")
        logger.info("=" * 50)

    def _convert_to_html(self, source_path: Path) -> Path | None:
        """æ‰§è¡Œ Excel åˆ° HTML è½¬æ¢"""
        logger.info("ğŸ“Œ ç¬¬ä¸€æ­¥ï¼šExcel è½¬ HTMLï¼ˆå¢å¼ºç‰ˆï¼‰")

        converter = ExcelToHtmlConverter(keywords=self.keywords)
        html_path = converter.convert(source_path)

        if not html_path:
            logger.error("æµæ°´çº¿ä¸­æ–­ï¼šHTML è½¬æ¢å¤±è´¥")
            return None

        return html_path

    def _chunk_html(self, html_path: Path, source_path: Path) -> ConversionResult | None:
        """æ‰§è¡Œ HTML åˆ‡åˆ†"""
        logger.info("ğŸ“Œ ç¬¬äºŒæ­¥ï¼šHTML åˆ‡åˆ†ä¸º Chunks")

        html_content = html_path.read_text(encoding="utf-8")

        config = self._build_chunk_config()
        chunker = HtmlChunker(config=config)
        result = chunker.chunk(html_content)

        self._log_chunk_result(result)

        # ä¿å­˜ç»“æœ
        chunk_path = source_path.parent / f"{source_path.stem}.html"
        return self._save_chunks(chunk_path, result, html_path)

    def _build_chunk_config(self) -> ChunkConfig:
        """æ„å»ºåˆ‡åˆ†é…ç½®"""
        if self.max_rows_per_chunk is None:
            logger.info(f"ğŸ“Š ä½¿ç”¨ token æ¨¡å¼ï¼Œç›®æ ‡æ¯ chunk â‰¤ {self.target_tokens} tokens")
            return ChunkConfig(
                split_mode=SplitMode.BY_TOKENS,
                max_tokens=self.target_tokens,
                separator=self.separator,
            )
        else:
            logger.info(f"ğŸ“Š ä½¿ç”¨è¡Œæ•°æ¨¡å¼ï¼Œæ¯ chunk {self.max_rows_per_chunk} è¡Œ")
            return ChunkConfig(
                split_mode=SplitMode.BY_ROWS,
                max_rows=self.max_rows_per_chunk,
                separator=self.separator,
            )

    def _log_chunk_result(self, result) -> None:
        """è®°å½•åˆ‡åˆ†ç»“æœ"""
        stats = result.stats
        logger.info(f"ğŸ”ª åˆ‡åˆ†å®Œæˆï¼šå…±ç”Ÿæˆ {stats.total_chunks} ä¸ªç‰‡æ®µ")
        logger.info(
            f"ğŸ“Š Token ç»Ÿè®¡: æœ€å°={stats.min_token_count}, "
            f"æœ€å¤§={stats.max_token_count}, å¹³å‡={stats.avg_token_count:.1f}"
        )

        if result.warnings:
            logger.warning(f"æœ‰ {len(result.warnings)} ä¸ªç‰‡æ®µè¶…è¿‡ token é™åˆ¶ï¼š")
            for w in result.warnings:
                logger.warning(
                    f"   - ç‰‡æ®µ #{w.chunk_index}: {w.actual_tokens} tokens (è¶…å‡º {w.overflow})"
                )
                logger.warning(f"     åŸå› : {w.reason}")

    def _save_chunks(self, chunk_path: Path, result, html_path: Path) -> ConversionResult | None:
        """ä¿å­˜åˆ‡åˆ†ç»“æœ"""
        formatted_separator = f"\n\n{self.separator}\n\n"
        merged_content = formatted_separator.join(result.chunks)

        try:
            chunk_path.write_text(merged_content, encoding="utf-8")
            logger.info(f"âœ… Chunk æ–‡ä»¶å·²ä¿å­˜: {chunk_path.absolute()}")
        except OSError as e:
            logger.error(f"å†™å…¥ Chunk æ–‡ä»¶å¤±è´¥: {e}")
            return None

        self._log_completion(html_path, chunk_path, result)

        return ConversionResult(
            html_path=html_path,
            chunk_path=chunk_path,
            chunk_count=len(result.chunks),
            status_message="å¤„ç†å®Œæˆ",
            success=True,
        )

    def _log_completion(self, html_path: Path, chunk_path: Path, result) -> None:
        """è®°å½•å®Œæˆæ—¥å¿—"""
        logger.info("=" * 50)
        logger.info("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        logger.info(f"   ğŸ“„ ä¸­é—´ç»“æœ (HTML): {html_path}")
        logger.info(f"   ğŸ“„ æœ€ç»ˆç»“æœ (Chunks): {chunk_path}")
        logger.info(f"   ğŸ”¢ Chunk æ•°é‡: {len(result.chunks)}")
        logger.info(f"   ğŸ”‘ åˆ†éš”ç¬¦: {self.separator}")
        logger.info("=" * 50)


def run_pipeline(
    excel_path: str,
    keywords: list[str] | None = None,
    max_rows_per_chunk: int | None = None,
    target_tokens: int = 1024,
    separator: str = "!!!_CHUNK_BREAK_!!!",
) -> dict | None:
    """æ‰§è¡Œå®Œæ•´çš„ Excel -> HTML -> Chunks æµæ°´çº¿ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    pipeline = ConversionPipeline(
        keywords=keywords,
        max_rows_per_chunk=max_rows_per_chunk,
        target_tokens=target_tokens,
        separator=separator,
    )

    result = pipeline.run(Path(excel_path))

    if result is None:
        return None

    return {
        "html_path": str(result.html_path),
        "chunk_path": str(result.chunk_path),
        "chunk_count": result.chunk_count,
    }


def main() -> None:
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="Excel è½¬æ¢æµæ°´çº¿ï¼ˆæ”¯æŒ HTML/MD/CSV æ ¼å¼ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python pipeline.py input.xlsx
  python pipeline.py input.xlsx -f md
  python pipeline.py input.xlsx -f csv --delimiter ";"
  python pipeline.py input.xlsx -k "è´¢åŠ¡æŠ¥è¡¨" "å¹´åº¦æ”¶å…¥"
  python pipeline.py input.xlsx -t 1024
  python pipeline.py input.xlsx -r 5
  python pipeline.py input.xlsx -t 2048 -s "---SPLIT---"
        """,
    )
    parser.add_argument("excel_file", nargs="+", help="è¦è½¬æ¢çš„ Excel æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    parser.add_argument(
        "-f",
        "--format",
        choices=["html", "md", "csv"],
        default="html",
        help="è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤: htmlï¼‰",
    )
    parser.add_argument("-k", "--keywords", nargs="+", help="å…³é”®æ£€ç´¢è¯ï¼ˆç”¨äºå¹½çµæ ‡é¢˜ï¼‰")
    parser.add_argument(
        "-r",
        "--max-rows",
        type=int,
        default=None,
        help="æ¯ä¸ª chunk çš„æœ€å¤§æ•°æ®è¡Œæ•°",
    )
    parser.add_argument(
        "-t",
        "--target-tokens",
        type=int,
        default=1024,
        help="ç›®æ ‡ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰",
    )
    parser.add_argument(
        "-s",
        "--separator",
        default="!!!_CHUNK_BREAK_!!!",
        help="chunk ä¹‹é—´çš„åˆ†éš”ç¬¦",
    )
    # CSV ç‰¹å®šå‚æ•°
    parser.add_argument(
        "--delimiter",
        default=",",
        help="CSV åˆ†éš”ç¬¦ï¼ˆé»˜è®¤: é€—å·ï¼‰",
    )
    parser.add_argument(
        "--encoding",
        default="utf-8",
        help="CSV ç¼–ç ï¼ˆé»˜è®¤: utf-8ï¼‰",
    )

    args = parser.parse_args()

    # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
    for excel_file in args.excel_file:
        if args.format == "html":
            run_pipeline(
                excel_path=excel_file,
                keywords=args.keywords,
                max_rows_per_chunk=args.max_rows,
                target_tokens=args.target_tokens,
                separator=args.separator,
            )
        else:
            from ..unified_pipeline import run_unified_pipeline
            run_unified_pipeline(
                excel_path=excel_file,
                output_format=args.format,
                keywords=args.keywords,
                max_rows_per_chunk=args.max_rows,
                target_tokens=args.target_tokens,
                separator=args.separator,
                csv_delimiter=args.delimiter,
                csv_encoding=args.encoding,
            )


if __name__ == "__main__":
    main()
