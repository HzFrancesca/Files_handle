"""
Excel è½¬ HTML å®Œæ•´æµæ°´çº¿
è¾“å…¥ Excel æ–‡ä»¶ -> ç”Ÿæˆå¢å¼º HTML -> åˆ‡åˆ†ä¸º Chunks
"""

from pathlib import Path
import argparse

from .excel2html_openpyxl_enhanced import convert_excel_to_html
from .html2chunk import distribute_assets_and_chunk, estimate_tokens


def estimate_rows_for_token_limit(html_content: str, target_tokens: int = 1024) -> int:
    """æ ¹æ®ç›®æ ‡ token æ•°ä¼°ç®—æ¯ä¸ª chunk åº”è¯¥åŒ…å«å¤šå°‘è¡Œ"""
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html_content, "html.parser")

    fixed_parts = []

    context_div = soup.find("div", class_="rag-context")
    if context_div:
        fixed_parts.append(str(context_div))

    table = soup.find("table")
    if not table:
        return 8

    caption = table.find("caption")
    if caption:
        fixed_parts.append(str(caption))

    thead = table.find("thead")
    if thead:
        fixed_parts.append(str(thead))

    fixed_overhead = estimate_tokens("".join(fixed_parts))

    tbody = table.find("tbody")
    if tbody:
        data_rows = tbody.find_all("tr")
    else:
        all_rows = table.find_all("tr")
        data_rows = all_rows[1:] if len(all_rows) > 1 else all_rows

    if not data_rows:
        return 8

    total_row_tokens = sum(estimate_tokens(str(row)) for row in data_rows)
    avg_tokens_per_row = total_row_tokens / len(data_rows)

    available_tokens = target_tokens - fixed_overhead

    if available_tokens <= 0 or avg_tokens_per_row <= 0:
        return 1

    suggested_rows = int(available_tokens / avg_tokens_per_row)

    return max(1, min(suggested_rows, 20))


def run_pipeline(
    excel_path: str,
    keywords: list = None,
    max_rows_per_chunk: int = None,
    target_tokens: int = 1024,
    separator: str = "!!!_CHUNK_BREAK_!!!",
):
    """æ‰§è¡Œå®Œæ•´çš„ Excel -> HTML -> Chunks æµæ°´çº¿"""
    source_path = Path(excel_path)

    if not source_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{source_path}'")
        return None

    print("=" * 50)
    print(f"ğŸš€ å¼€å§‹å¤„ç†æµæ°´çº¿: {source_path.name}")
    print("=" * 50)

    # === ç¬¬ä¸€æ­¥ï¼šExcel -> HTML ===
    print("\nğŸ“Œ ç¬¬ä¸€æ­¥ï¼šExcel è½¬ HTMLï¼ˆå¢å¼ºç‰ˆï¼‰")
    html_path = convert_excel_to_html(
        excel_path=str(source_path),
        keywords=keywords,
        output_path=None,
    )

    if not html_path:
        print("âŒ æµæ°´çº¿ä¸­æ–­ï¼šHTML è½¬æ¢å¤±è´¥")
        return None

    # === ç¬¬äºŒæ­¥ï¼šHTML -> Chunks ===
    print("\nğŸ“Œ ç¬¬äºŒæ­¥ï¼šHTML åˆ‡åˆ†ä¸º Chunks")
    html_content = Path(html_path).read_text(encoding="utf-8")

    if max_rows_per_chunk is None:
        print(f"ğŸ“Š ä½¿ç”¨ token æ¨¡å¼ï¼Œç›®æ ‡æ¯ chunk â‰¤ {target_tokens} tokens")
        result = distribute_assets_and_chunk(
            html_content,
            max_rows_per_chunk=None,
            max_tokens_per_chunk=target_tokens
        )
    else:
        print(f"ğŸ“Š ä½¿ç”¨è¡Œæ•°æ¨¡å¼ï¼Œæ¯ chunk {max_rows_per_chunk} è¡Œ")
        result = distribute_assets_and_chunk(
            html_content,
            max_rows_per_chunk=max_rows_per_chunk,
            max_tokens_per_chunk=None
        )
    
    chunks = result["chunks"]
    warnings = result["warnings"]
    stats = result["stats"]
    
    print(f"ğŸ”ª åˆ‡åˆ†å®Œæˆï¼šå…±ç”Ÿæˆ {stats['total_chunks']} ä¸ªç‰‡æ®µ")
    print(f"ğŸ“Š Token ç»Ÿè®¡: æœ€å°={stats['min_token_count']}, æœ€å¤§={stats['max_token_count']}, å¹³å‡={stats['avg_token_count']:.1f}")
    
    # è¾“å‡ºè¶…é™è­¦å‘Š
    if warnings:
        print(f"\nâš ï¸  è­¦å‘Šï¼šæœ‰ {len(warnings)} ä¸ªç‰‡æ®µè¶…è¿‡ token é™åˆ¶ï¼š")
        for w in warnings:
            print(f"   - ç‰‡æ®µ #{w['chunk_index']}: {w['actual_tokens']} tokens (è¶…å‡º {w['overflow']})")
            print(f"     åŸå› : {w['reason']}")

    chunk_path = source_path.parent / f"{source_path.stem}.html"

    formatted_separator = f"\n\n{separator}\n\n"
    merged_content = formatted_separator.join(chunks)

    try:
        chunk_path.write_text(merged_content, encoding="utf-8")
        print(f"âœ… Chunk æ–‡ä»¶å·²ä¿å­˜: {chunk_path.absolute()}")
    except IOError as e:
        print(f"âŒ å†™å…¥ Chunk æ–‡ä»¶å¤±è´¥: {e}")
        return None

    print("\n" + "=" * 50)
    print("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
    print(f"   ğŸ“„ ä¸­é—´ç»“æœ (HTML): {html_path}")
    print(f"   ğŸ“„ æœ€ç»ˆç»“æœ (Chunks): {chunk_path}")
    print(f"   ğŸ”¢ Chunk æ•°é‡: {len(chunks)}")
    print(f"   ğŸ”‘ åˆ†éš”ç¬¦: {separator}")
    print("=" * 50)

    return {
        "html_path": html_path,
        "chunk_path": str(chunk_path),
        "chunk_count": len(chunks),
    }


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="Excel è½¬ HTML å®Œæ•´æµæ°´çº¿ï¼ˆå¢å¼ºç‰ˆ + Chunk åˆ‡åˆ†ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python pipeline.py input.xlsx
  python pipeline.py input.xlsx -k "è´¢åŠ¡æŠ¥è¡¨" "å¹´åº¦æ”¶å…¥"
  python pipeline.py input.xlsx -t 1024
  python pipeline.py input.xlsx -r 5
  python pipeline.py input.xlsx -t 2048 -s "---SPLIT---"
        """,
    )
    parser.add_argument("excel_file", help="è¦è½¬æ¢çš„ Excel æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "-k", "--keywords", nargs="+", help="å…³é”®æ£€ç´¢è¯ï¼ˆç”¨äºå¹½çµæ ‡é¢˜ï¼‰"
    )
    parser.add_argument(
        "-r",
        "--max-rows",
        type=int,
        default=None,
        help="æ¯ä¸ª chunk çš„æœ€å¤§æ•°æ®è¡Œæ•°",
    )
    parser.add_argument(
        "-t",
        "--target-tokens",
        type=int,
        default=1024,
        help="ç›®æ ‡ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰",
    )
    parser.add_argument(
        "-s",
        "--separator",
        default="!!!_CHUNK_BREAK_!!!",
        help="chunk ä¹‹é—´çš„åˆ†éš”ç¬¦",
    )

    args = parser.parse_args()

    run_pipeline(
        excel_path=args.excel_file,
        keywords=args.keywords,
        max_rows_per_chunk=args.max_rows,
        target_tokens=args.target_tokens,
        separator=args.separator,
    )


if __name__ == "__main__":
    main()
