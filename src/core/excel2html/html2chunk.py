from pathlib import Path
from bs4 import BeautifulSoup
import copy
import json
import re
import tiktoken

# åˆå§‹åŒ– tokenizerï¼ˆcl100k_base ç”¨äº GPT-4/GPT-3.5-turboï¼‰
_tokenizer = None

def get_tokenizer():
    """æ‡’åŠ è½½ tokenizer"""
    global _tokenizer
    if _tokenizer is None:
        _tokenizer = tiktoken.get_encoding("cl100k_base")
    return _tokenizer


def estimate_tokens(text: str) -> int:
    """ä½¿ç”¨ tiktoken ç²¾ç¡®è®¡ç®— token æ•°é‡"""
    return len(get_tokenizer().encode(text))


def extract_note_references(text):
    """ä»æ–‡æœ¬ä¸­æå–æ³¨é‡Šå¼•ç”¨
    
    æ”¯æŒæ ¼å¼ï¼š
    - [æ³¨1] å•ä¸ªæ³¨é‡Š
    - [æ³¨1][æ³¨2] è¿ç»­å¤šä¸ªæ³¨é‡Š
    - [æ³¨1ã€2ã€3] æˆ– [æ³¨1,2,3] åˆå¹¶æ ¼å¼
    - æ³¨1 æ— æ–¹æ‹¬å·æ ¼å¼
    """
    refs = set()
    
    # 1. åŒ¹é…åˆå¹¶æ ¼å¼: [æ³¨1ã€2ã€3] æˆ– [æ³¨1,2,3]
    multi_refs = re.findall(r'\[(æ³¨)([\dã€,ï¼Œ]+)\]', text)
    for prefix, nums_str in multi_refs:
        nums = re.split(r'[ã€,ï¼Œ]', nums_str)
        for num in nums:
            num = num.strip()
            if num:
                refs.add(f"{prefix}{num}")
    
    # 2. åŒ¹é…å•ä¸ªæ–¹æ‹¬å·æ³¨é‡Š: [æ³¨1], [å¤‡æ³¨], [è¯´æ˜2] ç­‰
    bracket_refs = re.findall(r'\[(æ³¨\s*\d*|å¤‡æ³¨\s*\d*|è¯´æ˜\s*\d*|æ³¨æ„\s*\d*)\s*\]', text)
    refs.update(ref.replace(' ', '') for ref in bracket_refs)
    
    # 3. åŒ¹é…æ— æ–¹æ‹¬å·çš„æ³¨é‡Šå¼•ç”¨: æ•°å€¼æ³¨1
    superscript_refs = re.findall(r'[^\[](æ³¨\d+)(?:[ï¼š:ï¼‰\)]|$|\s)', text)
    refs.update(superscript_refs)
    
    # 4. ç‰¹æ®Šç¬¦å·
    if '*' in text:
        refs.add('*')
    if 'â€»' in text:
        refs.add('â€»')
    
    return refs


def distribute_assets_and_chunk(
    long_html_content, max_rows_per_chunk: int = None, max_tokens_per_chunk: int = None,
    min_tokens_per_chunk: int = None, token_strategy: str = "prefer_max"
):
    """
    æ ¸å¿ƒé€»è¾‘ï¼šå°†é•¿ HTML åˆ‡åˆ†ï¼Œå¹¶æŠŠå…¨å±€èµ„äº§ï¼ˆContext/Caption/Headerï¼‰åˆ†å‘ç»™æ¯ä¸ªç‰‡æ®µ

    å‚æ•°:
        long_html_content: å®Œæ•´ HTML å†…å®¹
        max_rows_per_chunk: æŒ‰è¡Œæ•°åˆ‡åˆ†ï¼ˆä¼˜å…ˆï¼‰
        max_tokens_per_chunk: æŒ‰ token æ•°åˆ‡åˆ†ï¼ˆæ›´ç²¾ç¡®ï¼‰- æœ€å¤§ token é™åˆ¶
        min_tokens_per_chunk: æœ€å° token æ•°ï¼ˆå¯é€‰ï¼‰- chunk è‡³å°‘è¦è¾¾åˆ°æ­¤å€¼æ‰åˆ‡åˆ†
        token_strategy: åˆ‡åˆ†ç­–ç•¥ï¼ˆä»…åœ¨å¯ç”¨ min_tokens_per_chunk æ—¶ç”Ÿæ•ˆï¼‰
            - "prefer_max": æ¥è¿‘æœ€å¤§å€¼ - ç´¯åŠ åˆ°æ¥è¿‘ max_tokens æ‰åˆ‡åˆ†ï¼ˆé»˜è®¤ï¼‰
            - "prefer_min": æ¥è¿‘æœ€å°å€¼ - è¶…è¿‡ min_tokens å°±ç«‹å³åˆ‡åˆ†

    å¦‚æœä¸¤ä¸ªå‚æ•°éƒ½æœªæŒ‡å®šï¼Œé»˜è®¤ max_rows_per_chunk=8
    
    è¿”å›:
        dict: {
            "chunks": list[str],  # åˆ‡åˆ†åçš„ HTML ç‰‡æ®µ
            "warnings": list[dict],  # è¶…é™è­¦å‘Šä¿¡æ¯
            "stats": dict  # ç»Ÿè®¡ä¿¡æ¯
        }
    """
    if max_rows_per_chunk is None and max_tokens_per_chunk is None:
        max_rows_per_chunk = 8
    
    # éªŒè¯ min_tokens_per_chunk å‚æ•°
    if min_tokens_per_chunk is not None:
        if max_tokens_per_chunk is None:
            raise ValueError("min_tokens_per_chunk åªèƒ½åœ¨ token æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œè¯·åŒæ—¶è®¾ç½® max_tokens_per_chunk")
        if min_tokens_per_chunk >= max_tokens_per_chunk:
            raise ValueError(f"min_tokens_per_chunk ({min_tokens_per_chunk}) å¿…é¡»å°äº max_tokens_per_chunk ({max_tokens_per_chunk})")

    soup = BeautifulSoup(long_html_content, "html.parser")

    # 1. æå–å…¨å±€èµ„äº§ (Context Div)
    context_div = soup.find("div", class_="rag-context")
    if not context_div:
        table_node = soup.find("table")
        if table_node and table_node.find_previous_sibling("div"):
            context_div = table_node.find_previous_sibling("div")

    # 1.1 æå–æ³¨é‡Šå…ƒæ•°æ®
    notes_meta_script = soup.find("script", class_="table-notes-meta")
    header_notes = {}
    conditional_notes = {}
    if notes_meta_script:
        try:
            notes_meta = json.loads(notes_meta_script.string)
            header_notes = notes_meta.get("header_notes", {})
            conditional_notes = notes_meta.get("conditional_notes", {})
        except (json.JSONDecodeError, AttributeError):
            pass

    # 2. æå–è¡¨æ ¼æ ¸å¿ƒç»„ä»¶
    original_table = soup.find("table")
    if not original_table:
        return {
            "chunks": [long_html_content],
            "warnings": [],
            "stats": {"total_chunks": 1, "oversized_chunks": 0}
        }

    caption = original_table.find("caption")

    header_rows = []
    thead = original_table.find("thead")
    if thead:
        header_rows = thead.find_all("tr")
    else:
        header_rows = original_table.find_all("tr")[:1]

    # 3. å‡†å¤‡æ•°æ®è¡Œï¼ˆæ’é™¤æ³¨é‡Šè¡Œï¼‰
    tbody = original_table.find("tbody")
    if tbody:
        all_body_rows = tbody.find_all("tr")
        data_rows = [row for row in all_body_rows if "table-note-row" not in row.get("class", [])]
    else:
        all_rows = original_table.find_all("tr")
        data_rows = [row for row in all_rows if row not in header_rows]

    # 4. è®¡ç®—åŸºç¡€å›ºå®šå¼€é”€ï¼ˆç”¨äº token æ¨¡å¼ï¼‰
    fixed_parts = []
    if context_div:
        fixed_parts.append(str(context_div))
    if caption:
        fixed_parts.append(str(caption))
    for h_row in header_rows:
        fixed_parts.append(str(h_row))
    base_fixed_overhead = estimate_tokens("".join(fixed_parts))
    
    # 4.1 é¢„è®¡ç®— header_notes çš„å›ºå®šå¼€é”€ï¼ˆæ¯ä¸ª chunk éƒ½ä¼šæ·»åŠ ï¼‰
    header_notes_text = " | ".join(header_notes.values()) if header_notes else ""
    header_notes_overhead = estimate_tokens(header_notes_text) if header_notes_text else 0
    
    # é¢„è®¡ç®—è¡¨å¤´æ–‡æœ¬ï¼ˆç”¨äºæ³¨é‡Šå¼•ç”¨åŒ¹é…ï¼‰
    header_text = " ".join(str(row) for row in header_rows)

    chunks = []
    chunk_token_counts = []  # è®°å½•æ¯ä¸ª chunk çš„å®é™… token æ•°
    warnings = []  # è¶…é™è­¦å‘Š
    current_chunk_data = []
    current_chunk_tokens = 0

    def calculate_notes_overhead(pending_rows):
        """åŠ¨æ€è®¡ç®—å½“å‰ chunk å®é™…ä¼šåŒ¹é…çš„æ³¨é‡Š token å¼€é”€"""
        if not header_notes and not conditional_notes:
            return 0
        
        # ä»å¾…å¤„ç†è¡Œå’Œè¡¨å¤´ä¸­æå–æ³¨é‡Šå¼•ç”¨
        chunk_text = " ".join(str(row) for row in pending_rows)
        all_text = chunk_text + " " + header_text
        chunk_refs = extract_note_references(all_text)
        
        # æ”¶é›†å®é™…ä¼šæ·»åŠ çš„æ³¨é‡Šï¼ˆå»é‡ï¼‰
        actual_notes = []
        seen_notes = set()
        for note in header_notes.values():
            if note not in seen_notes:
                actual_notes.append(note)
                seen_notes.add(note)
        for key, note in conditional_notes.items():
            if key in chunk_refs and note not in seen_notes:
                actual_notes.append(note)
                seen_notes.add(note)
        
        if not actual_notes:
            return 0
        
        notes_text = " | ".join(actual_notes)
        return estimate_tokens(f" ã€è¡¨æ ¼æ³¨é‡Šã€‘{notes_text}")

    def should_split(row_count, row_tokens, pending_rows, new_row):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡åˆ†ï¼ˆåŠ¨æ€è®¡ç®—æ³¨é‡Šå¼€é”€ï¼‰
        
        è¿”å› True è¡¨ç¤ºï¼šåœ¨åŠ å…¥ new_row ä¹‹å‰ï¼Œå…ˆæŠŠ pending_rows è¾“å‡ºä¸ºä¸€ä¸ª chunk
        
        ç­–ç•¥è¯´æ˜ï¼š
        - prefer_max: å°½é‡ç´¯ç§¯åˆ°æ¥è¿‘ max_tokensï¼Œåªæœ‰åŠ å…¥æ–°è¡Œä¼šè¶…è¿‡ max æ—¶æ‰åˆ‡åˆ†
        - prefer_min: åªè¦å½“å‰å·²è¾¾åˆ° min_tokensï¼Œå°±å¯ä»¥åˆ‡åˆ†ï¼ˆä½†ä¸èƒ½è¶…è¿‡ maxï¼‰
        """
        if max_tokens_per_chunk is not None:
            # è®¡ç®—å½“å‰ chunk çš„ token æ•°ï¼ˆä¸å«æ–°è¡Œï¼‰
            current_notes_overhead = calculate_notes_overhead(pending_rows)
            current_total = current_chunk_tokens + base_fixed_overhead + current_notes_overhead
            
            # è®¡ç®—å¦‚æœåŠ å…¥æ–°è¡Œåçš„æ€» token æ•°
            test_rows = pending_rows + [new_row]
            notes_overhead = calculate_notes_overhead(test_rows)
            total_overhead = base_fixed_overhead + notes_overhead
            potential_total = current_chunk_tokens + row_tokens + total_overhead
            
            # å¦‚æœåŠ å…¥æ–°è¡Œä¼šè¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œå¿…é¡»åˆ‡åˆ†
            if potential_total > max_tokens_per_chunk:
                return True
            
            # å¦‚æœå¯ç”¨äº†æœ€å° token é™åˆ¶ä¸”ä½¿ç”¨ prefer_min ç­–ç•¥
            if min_tokens_per_chunk is not None and token_strategy == "prefer_min":
                # å½“å‰ chunk å·²è¾¾åˆ°æœ€å°å€¼ï¼Œå¯ä»¥åˆ‡åˆ†
                if current_total >= min_tokens_per_chunk:
                    return True
            
            # å…¶ä»–æƒ…å†µï¼ˆprefer_max æˆ–æœªè¾¾åˆ° minï¼‰ï¼šç»§ç»­ç´¯ç§¯
            return False
        else:
            return row_count >= max_rows_per_chunk

    def build_chunk(data_rows_for_chunk):
        """ç»„è£…ä¸€ä¸ª chunkï¼Œæ™ºèƒ½æ·»åŠ åŒ¹é…çš„æ³¨é‡Š"""
        new_soup = BeautifulSoup("<div></div>", "html.parser")
        wrapper_div = new_soup.div

        # ä»æ•°æ®è¡Œå’Œè¡¨å¤´ä¸­æå–æ³¨é‡Šå¼•ç”¨
        chunk_text = " ".join(str(row) for row in data_rows_for_chunk)
        header_text = " ".join(str(row) for row in header_rows)
        all_text = chunk_text + " " + header_text
        chunk_refs = extract_note_references(all_text)
        
        matched_notes = []
        seen_notes = set()  # ç”¨äºå»é‡
        for key, note in header_notes.items():
            if note not in seen_notes:
                matched_notes.append(note)
                seen_notes.add(note)
        for key, note in conditional_notes.items():
            if key in chunk_refs and note not in seen_notes:
                matched_notes.append(note)
                seen_notes.add(note)
        
        if context_div:
            new_context = copy.copy(context_div)
            if matched_notes:
                notes_text = " | ".join(matched_notes)
                new_context.string = (new_context.get_text() or "") + f" ã€è¡¨æ ¼æ³¨é‡Šã€‘{notes_text}"
            wrapper_div.append(new_context)

        new_table = new_soup.new_tag("table")
        new_table.attrs = original_table.attrs
        new_table["border"] = "1"
        new_table["style"] = "border-collapse:collapse"

        if caption:
            new_table.append(copy.copy(caption))

        new_thead = new_soup.new_tag("thead")
        for h_row in header_rows:
            new_thead.append(copy.copy(h_row))
        new_table.append(new_thead)

        new_tbody = new_soup.new_tag("tbody")
        for d_row in data_rows_for_chunk:
            new_tbody.append(copy.copy(d_row))
        new_table.append(new_tbody)

        wrapper_div.append(new_table)
        return str(wrapper_div)

    # 5. é€è¡Œç´¯åŠ åˆ‡åˆ†
    for i, row in enumerate(data_rows):
        row_tokens = estimate_tokens(str(row))

        if current_chunk_data and should_split(len(current_chunk_data), row_tokens, current_chunk_data, row):
            # è®°å½•å½“å‰ chunk çš„ token æ•°
            final_notes_overhead = calculate_notes_overhead(current_chunk_data)
            final_total = current_chunk_tokens + base_fixed_overhead + final_notes_overhead
            chunk_token_counts.append(final_total)
            chunks.append(build_chunk(current_chunk_data))
            current_chunk_data = []
            current_chunk_tokens = 0

        current_chunk_data.append(row)
        current_chunk_tokens += row_tokens
        
        # æ£€æŸ¥å½“å‰ chunk æ˜¯å¦å·²è¶…é™ï¼ˆå¤„ç†å•è¡Œè¶…é™çš„æƒ…å†µï¼‰
        if max_tokens_per_chunk is not None:
            current_notes_overhead = calculate_notes_overhead(current_chunk_data)
            current_total = current_chunk_tokens + base_fixed_overhead + current_notes_overhead
            if current_total > max_tokens_per_chunk:
                # è®°å½•è¶…é™è­¦å‘Š
                chunk_index = len(chunks)
                warnings.append({
                    "chunk_index": chunk_index,
                    "actual_tokens": current_total,
                    "limit": max_tokens_per_chunk,
                    "overflow": current_total - max_tokens_per_chunk,
                    "row_count": len(current_chunk_data),
                    "reason": "å•è¡Œæ•°æ® + å›ºå®šå¼€é”€ + æ³¨é‡Šè¶…è¿‡ token é™åˆ¶" if len(current_chunk_data) == 1 else "ç´¯ç§¯æ•°æ®è¶…è¿‡ token é™åˆ¶"
                })
                # å½“å‰ chunk å·²è¶…é™ï¼Œç«‹å³è¾“å‡º
                chunk_token_counts.append(current_total)
                chunks.append(build_chunk(current_chunk_data))
                current_chunk_data = []
                current_chunk_tokens = 0

        if i == len(data_rows) - 1 and current_chunk_data:
            final_notes_overhead = calculate_notes_overhead(current_chunk_data)
            final_total = current_chunk_tokens + base_fixed_overhead + final_notes_overhead
            chunk_token_counts.append(final_total)
            chunks.append(build_chunk(current_chunk_data))

    # æ„å»ºç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_chunks": len(chunks),
        "oversized_chunks": len(warnings),
        "token_counts": chunk_token_counts,
        "max_token_count": max(chunk_token_counts) if chunk_token_counts else 0,
        "min_token_count": min(chunk_token_counts) if chunk_token_counts else 0,
        "avg_token_count": sum(chunk_token_counts) / len(chunk_token_counts) if chunk_token_counts else 0,
        "base_fixed_overhead": base_fixed_overhead,
    }
    
    if max_tokens_per_chunk:
        stats["token_limit"] = max_tokens_per_chunk
    if min_tokens_per_chunk:
        stats["min_token_limit"] = min_tokens_per_chunk
        stats["token_strategy"] = token_strategy

    return {
        "chunks": chunks,
        "warnings": warnings,
        "stats": stats
    }


def process_and_merge_html(file_path_str, separator="!!!_CHUNK_BREAK_!!!"):
    """è¯»å–æ–‡ä»¶ -> åˆ‡åˆ† -> åˆå¹¶ -> ä¿å­˜"""
    source_path = Path(file_path_str)

    if not source_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{source_path}'")
        return

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–: {source_path.name}")
    content = source_path.read_text(encoding="utf-8")

    result = distribute_assets_and_chunk(content, max_rows_per_chunk=2)
    chunks = result["chunks"]
    warnings = result["warnings"]
    stats = result["stats"]

    print(f"ğŸ”ª åˆ‡åˆ†å®Œæˆï¼šå…±ç”Ÿæˆ {stats['total_chunks']} ä¸ªç‰‡æ®µ")
    print(f"ğŸ“Š Token ç»Ÿè®¡: æœ€å°={stats['min_token_count']}, æœ€å¤§={stats['max_token_count']}, å¹³å‡={stats['avg_token_count']:.1f}")
    
    # è¾“å‡ºè¶…é™è­¦å‘Š
    if warnings:
        print(f"\nâš ï¸  è­¦å‘Šï¼šæœ‰ {len(warnings)} ä¸ªç‰‡æ®µè¶…è¿‡ token é™åˆ¶ï¼š")
        for w in warnings:
            print(f"   - ç‰‡æ®µ #{w['chunk_index']}: {w['actual_tokens']} tokens (è¶…å‡º {w['overflow']})")
            print(f"     åŸå› : {w['reason']}")

    formatted_separator = f"\n\n{separator}\n\n"
    merged_content = formatted_separator.join(chunks)

    new_filename = source_path.stem + "_chunk_merged" + source_path.suffix
    output_path = source_path.with_name(new_filename)

    try:
        output_path.write_text(merged_content, encoding="utf-8")
        print(f"\nâœ… åˆå¹¶æˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path.absolute()}")
        print(f"ğŸ”‘ ä½¿ç”¨çš„åˆ†éš”ç¬¦: {separator}")
    except IOError as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")


if __name__ == "__main__":
    my_input_file = "Files\\Excel\\æœ¬å›½å­ç›®æ³¨é‡Šè°ƒæ•´è¡¨.html"
    process_and_merge_html(my_input_file)
